# Attention和KV Cache
## 内存受限和计算受限
计算带宽$BW_{math}$指的是处理器每秒钟可以执行的数学计算次数，单位通常是Flops。
内存带宽$BW_{mem}$指的是处理器每秒钟从内存中读取的数据量，单位是Bytes/s。
算数强度定义为：$\frac{N_{op}}{N_{byte}}$。

对于特定的硬件，其$\frac{BW_{math}} {BW_{mem}}$是固定的，比如A100-40GB的为201Flops/Bytes，
**以下的计算都以这个数据为基准，同时计算精度均选择半精度float16。**

对于特定的算子，其理论的算数强度也是确定的。
比如对于两个向量内积，需要$n$次乘法和$n$次加法，计算量为$2n$ Flops，半精度情况下，需要的内存大小为$4n$ Bytes，算数强度为$0.5$ Flops/Bytes。因此是一个内存受限的算子。

对于矩阵乘法，$A\in \mathbb{R^{m\times m}}, B\in \mathbb{R^{m\times m}}$，$AB$的计算量为$2m^3$ Flops，内存大小为$4m^2$ Bytes，算数强度为$0.5m$ Flops/Bytes，因此当$0.5m > 201$时，矩阵乘法会变成计算受限算子。

在LLM的情景中，矩阵乘法分为两种情况：QKV投影: $[L, D] \times [D, D]$，$QK^T$注意力分数: $[L, D] \times [D, L]$。

对于第一种情况：$[L, D] \times [D, D]$，$L$是序列长度，$D$的embedding的维度。这里我直接引用[知乎](https://zhuanlan.zhihu.com/p/639228219?s_r=0)的一张表，可以看出这部分往往是内存受限的。

| N    | d   | ops/bytes | 受限类型       |
|------|-----|-----------|----------------|
| 256  | 64  | 43        | <201, memory-bound |
| 2048 | 64  | 60        | <201, memory-bound |
| 4096 | 64  | 62        | <201, memory-bound |
| 256  | 128 | 64        | <201, memory-bound |
| 2048 | 128 | 114       | <201, memory-bound |
| 4096 | 128 | 120       | <201, memory-bound |
| 256  | 256 | 85        | <201, memory-bound |
| 2048 | 256 | 205       | >201, math-bound   |
| 4096 | 256 | 228       | >201, math-bound   |

对于第二种情况：$[L, D] \times [D, L]$，计算量为$2L^2 D$ Flops，内存大小为$4LD$ Bytes，算数强度为$0.5L$，而LLM中一般$L>2048$，所以这一部分显然是非常计算受限的。

因此可以看出，计算Attention的过程中，性能瓶颈主要就在QK注意力分数的计算，而这部分只是一个矩阵乘法，是一个已经被完完全全优化到头的算子，很难再有什么新优化了。

## KV Cache
由于LLM输入序列的因果性，$q_i$只能和$k_{0,...,i}$做注意力计算，不能和$k_{i+1,...,n-1}$做注意力，这点和CV中用的Attention是不一样的。这里直接用大佬的图：

![attn1](img/attn1.png)

可以看出，在正常的Attention流程中，对于新来的$x_i$，需要读取$x_{0,...,i}$并乘以$W_k$，得到$k_{0,...,i}$，而在上一个iter中已经计算了$k_{0,...,i-1}$，我们只需要把之前计算的结果缓存下来，这就是K Cache。

此时我们计算注意力分数的流程是：给定当前iter的输入$x_i$，计算$q_i = W_q x_i$，$k_i = W_k x_i$，$S_i = q_i [K_{i-1} | k_i]$。$K_{i-1}$是上一个iter中缓存的$K$矩阵，$S_i$是当前token的注意力分数，是最终QK矩阵的第$i$行。从而避免了对$k_{0,...,i-1}$的重复计算，以提升性能。

以上是利用K Cache减少注意力分数$S = QK^T$的计算量的过程，同理也可以利用V Cache减少加权$O = SV$的计算量。

对于LLAMA2-7B，其规模为：$N=32, D=128, H_{attn}=32, H_{kv}=8$，一个token对应的KV Cache大小就是$sizeof(fp16) \times N D H_{kv} \times 2 = 128KB$，上下文窗口为$L=4096$，故需要的总KV Cache大小为$4095 \times 128KB$，约为0.5GB。